{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f36d687",
   "metadata": {},
   "source": [
    "## 5.1 Evaluating generative text models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "00fdc89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "650263d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9cc731b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "70354c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "04e265ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "               torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * x ** 3)\n",
    "                                        )\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3ce547d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELUNew(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"_a\", torch.tensor(0.7978845608))  # sqrt(2/pi)\n",
    "        self.register_buffer(\"_b\", torch.tensor(0.044715))\n",
    "\n",
    "    def forward(self, x):\n",
    "        a = self._a.to(dtype=x.dtype)  # 与输入匹配 dtype\n",
    "        b = self._b.to(dtype=x.dtype)\n",
    "        return 0.5 * x * (1.0 + torch.tanh(a * (x + b * x * x * x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "183b2304",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "#         self.layers = nn.Sequential(nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), \n",
    "#                                    GELU(),\n",
    "#                                    nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]))\n",
    "\n",
    "        self.layers = nn.Sequential(nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), \n",
    "                                   GELUNew(),\n",
    "                                   nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "75bf9c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert(d_out % num_heads ==0), \"d_out must be divisible by num_heads\"  \n",
    "        \n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads  # 多头相当于将单头等比例拆分，计算后再合并\n",
    "        self.W_q = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_k = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_v = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = torch.nn.Linear(d_out, d_out)  # 合并后使信息交融\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        \n",
    "        keys = self.W_k(x)\n",
    "        queries = self.W_q(x)\n",
    "        values = self.W_v(x)\n",
    "        \n",
    "        # 进行reshape，分成多头\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        \n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        \n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        \n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
    "        attn_weights_dropout = self.dropout(attn_weights)\n",
    "        \n",
    "        context_vec = (attn_weights_dropout @ values).transpose(1, 2)  # 变回原顺序以便reshape回原来形状\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        \n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0b4807ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(  # 注意力模块\n",
    "            d_in = cfg[\"emb_dim\"],\n",
    "            d_out = cfg[\"emb_dim\"],\n",
    "            context_length = cfg[\"context_length\"],\n",
    "            num_heads = cfg[\"n_heads\"],\n",
    "            dropout = cfg[\"drop_rate\"],\n",
    "            qkv_bias = cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        \n",
    "        self.ff = FeedForward(cfg)  # 前向传播\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "        \n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d3466c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        self.trf_blocks = nn.Sequential(*[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head =  nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "        \n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        \n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        \n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a95e33d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPT_CONFIG_124M = {\n",
    " \"vocab_size\": 50257,\n",
    " \"context_length\": 256, \n",
    " \"emb_dim\": 768,\n",
    " \"n_heads\": 12,\n",
    " \"n_layers\": 12, \n",
    " \"drop_rate\": 0.1, \n",
    " \"qkv_bias\": False\n",
    "}\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a75e530a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        logits = logits[:, -1, :]\n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5ad34d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1c5cac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将文本转化为toekn id\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encode_tensor = torch.tensor(encoded).unsqueeze(0) # 增加1维\n",
    "    \n",
    "    return encode_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00a886b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将token id转化为文本\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)\n",
    "    flat_ls = flat.tolist()\n",
    "    ids_to_text = tokenizer.decode(flat_ls)\n",
    "    \n",
    "    return ids_to_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61c699c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_context = \"Every effort moves you\"\n",
    "token_ids = generate_text_simple(model=model, idx=text_to_token_ids(start_context, tokenizer), \n",
    "                                 max_new_tokens=10, context_size=GPT_CONFIG_124M['context_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ac360cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result: Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "gen_text = token_ids_to_text(token_ids, tokenizer)\n",
    "print(\"result:\", gen_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78b2bad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87add6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100], # [\"every effort moves\",\n",
    "                       [40, 1107, 588]]) # \"I really like\"]\n",
    "targets = torch.tensor([[3626, 6100, 345 ], # [\" effort moves you\",\n",
    "                        [1107, 588, 11311]]) # \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5db3a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "probas = torch.softmax(logits, dim=-1)\n",
    "print(probas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1cbd44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c6ffba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' effort moves you'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target\n",
    "token_ids_to_text(targets[0], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "129dc659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Armed heNetflix'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output\n",
    "token_ids_to_text(token_ids[0].flatten(), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82fcbd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([7.4541e-05, 3.1061e-05, 1.1563e-05])\n",
      "Text 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])\n"
     ]
    }
   ],
   "source": [
    "# 取出正确位置上的概率（输入只有3个token，所以中间是[0, 1, 2]）\n",
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2520c36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n"
     ]
    }
   ],
   "source": [
    "# 取对数\n",
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d205f361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.7940)\n"
     ]
    }
   ],
   "source": [
    "# 取均值\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc2b46f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "# 取相反数\n",
    "negative_avg_log_probas = avg_log_probas * -1\n",
    "print(negative_avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57b582b",
   "metadata": {},
   "source": [
    "以上手动实现了交叉熵损失函数的结果，优化的目标就是使得预测结果对应target位置上的概率最大，即接近1，此时log1=0，所以当交叉熵结果接近0时，预测值与真实值一致"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b576747",
   "metadata": {},
   "source": [
    "**手工实现交叉熵（cross-entropy）**。核心公式只有一条：\n",
    "\n",
    "$$\n",
    "\\text{CE} = -\\frac{1}{N}\\sum_{i=1}^{N}\\log p_\\theta\\big(y_i\\mid x_i\\big)\n",
    "$$\n",
    "\n",
    "也就是：对每个位置 $i$，“正确类别 $y_i$” 的**预测概率**取对数，再取负号并求平均。\n",
    "\n",
    "1. `probas = softmax(logits)`\n",
    "   得到每个位置对所有token的预测概率 $p_\\theta(\\cdot\\mid x_i)$。\n",
    "\n",
    "2. `probas[text_idx, [0,1,2], targets[text_idx]]`\n",
    "   这一步相当于用索引/`gather`把“每个位置上**正确token id**的概率”取出来，也就是\n",
    "   $[p_\\theta(y_0\\mid x_0),\\ p_\\theta(y_1\\mid x_1),\\ p_\\theta(y_2\\mid x_2)]$。\n",
    "\n",
    "3. `torch.cat((target_probas_1, target_probas_2))`\n",
    "   只是把**两条样本序列**（Text 1、Text 2）在同一维度上拼成一个更长的一维向量，方便**一次性**对所有位置求一个整体平均损失。等价地，你也可以分别算两段的loss再求平均；`cat`不改变数值含义，只是做了**拼接以便聚合**。\n",
    "\n",
    "4. `torch.log(...)`\n",
    "   对这些“正确类别的概率”取对数，得到 $\\log p_\\theta(y_i\\mid x_i)$。接下来取负号、再求平均，就是交叉熵/负对数似然（NLL）损失。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e4fbae",
   "metadata": {},
   "source": [
    "尝试使用pytorch内置的交叉熵函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c91ac2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(\"Logits shape:\", logits.shape)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "11d69989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 50257])\n",
      "torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flatten = logits.flatten(0, 1)\n",
    "targets_flatten = targets.flatten()\n",
    "print(logits_flatten.shape)\n",
    "print(targets_flatten.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a09ef69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.functional.cross_entropy(logits_flatten, targets_flatten)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419d0b13",
   "metadata": {},
   "source": [
    "使用Perplexity判断一致性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "49da3504",
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity = torch.exp(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3e0f7041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(48725.8203)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bba424",
   "metadata": {},
   "source": [
    "## 理解困惑度\n",
    "### 1. 从交叉熵出发\n",
    "\n",
    "对一个序列，交叉熵（平均负对数似然）是\n",
    "\n",
    "$$\n",
    "\\text{loss} = - \\frac{1}{N} \\sum_{i=1}^N \\log p_\\theta(y_i \\mid x_i)\n",
    "$$\n",
    "\n",
    "其中 $p_\\theta(y_i \\mid x_i)$ 是模型在第 $i$ 个位置对正确 token 的预测概率。\n",
    "\n",
    "---\n",
    "\n",
    "### 2. 困惑度定义\n",
    "\n",
    "$$\n",
    "\\text{PPL} = \\exp(\\text{loss})\n",
    "$$\n",
    "\n",
    "代进去：\n",
    "\n",
    "$$\n",
    "\\text{PPL} = \\exp\\!\\left(- \\frac{1}{N} \\sum_{i=1}^N \\log p_\\theta(y_i \\mid x_i)\\right)\n",
    "$$\n",
    "\n",
    "这其实就是**几何平均倒数**：\n",
    "\n",
    "$$\n",
    "\\text{PPL} = \\left(\\prod_{i=1}^N \\frac{1}{p_\\theta(y_i \\mid x_i)} \\right)^{1/N}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 3. 理解“等效候选数”\n",
    "\n",
    "如果模型在每个位置都是完全均匀分布，概率是：\n",
    "\n",
    "$$\n",
    "p_\\theta(y_i \\mid x_i) = \\frac{1}{M}\n",
    "$$\n",
    "\n",
    "其中 $M$ 是均匀候选数。\n",
    "\n",
    "那么：\n",
    "\n",
    "$$\n",
    "\\text{loss} = - \\log \\frac{1}{M} = \\log M\n",
    "$$\n",
    "\n",
    "再取指数：\n",
    "\n",
    "$$\n",
    "\\text{PPL} = \\exp(\\log M) = M\n",
    "$$\n",
    "\n",
    "👉 这说明 **困惑度正好等于均匀分布下的候选 token 数量**。\n",
    "\n",
    "---\n",
    "\n",
    "### 4. 推广到非均匀情况\n",
    "\n",
    "当模型分布不完全均匀时，PPL 就是那个“等效均匀分布的大小”。\n",
    "换句话说，它等价于：\n",
    "**如果有一个均匀分布能产生同样的不确定性，它的候选大小是多少？**\n",
    "\n",
    "---\n",
    "\n",
    "更精确地说：\n",
    "\n",
    "* **交叉熵（loss）** 度量了模型预测分布和真实分布之间的不确定性。\n",
    "* **困惑度（PPL） = exp(loss)** 就是把这个不确定性重新“翻译”成 **一个等价的均匀分布规模**。\n",
    "\n",
    "换句话说：\n",
    "\n",
    "* 在非均匀的实际预测里，不同 token 概率差异很大。\n",
    "* 但 PPL 会问：“如果有一个均匀分布，它的不确定性（熵）等价于模型当前的分布，那这个均匀分布有多少个候选 token？”\n",
    "\n",
    "于是你就能把复杂的概率分布，转化为一个直观的数值 —— **等效候选数量**。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4e133b",
   "metadata": {},
   "source": [
    "### Calculating the training and validation set losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c71e2349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_size': 50257,\n",
       " 'context_length': 256,\n",
       " 'emb_dim': 768,\n",
       " 'n_heads': 12,\n",
       " 'n_layers': 12,\n",
       " 'drop_rate': 0.1,\n",
       " 'qkv_bias': False}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPT_CONFIG_124M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e140d438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# cuda是否可用\n",
    "print(torch.cuda.is_available())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "65ed6ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"the-verdict.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e0c94b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "711afdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集和测试集\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e868372e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "900a2c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将文本转化为token_id并以tensor形式存储\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "        \n",
    "        for i in range(0, len(token_ids) - max_length, stride):  # 建立对应的输入和输出toekn_id tensor\n",
    "            input_chunk = token_ids[i: i+ max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)  # 判断长度\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]  # 返回输入和目标tensor中某一行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a0de48f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride) \n",
    "    dataloader = DataLoader(\n",
    "                            dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=shuffle,\n",
    "                            drop_last=drop_last, \n",
    "                            num_workers=num_workers \n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "693546d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2c25684bed0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "da42306e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = create_dataloader_v1(\n",
    " train_data,\n",
    " batch_size=2,\n",
    " max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    " stride=GPT_CONFIG_124M[\"context_length\"],\n",
    " drop_last=True,\n",
    " shuffle=True,\n",
    " num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    " val_data,\n",
    " batch_size=2,\n",
    " max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    " stride=GPT_CONFIG_124M[\"context_length\"],\n",
    " drop_last=False,\n",
    " shuffle=False,\n",
    " num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a5059d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3f92a07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算一个batch的loss\n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch = input_batch.to(device) \n",
    "    target_batch = target_batch.to(device) \n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "           logits.flatten(0, 1), target_batch.flatten()\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1ffd7ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算多个batch的损失\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    \n",
    "    # 得到batch数量\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader) \n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader)) \n",
    "        \n",
    "    # 累加每个batch的loss    \n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device\n",
    "            )\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # 返回每个batch平均loss\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ef0355e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.987583266364204\n",
      "Validation loss: 10.981104850769043\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device) \n",
    "with torch.no_grad(): \n",
    "    train_loss = calc_loss_loader(train_loader, model, device) \n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95923c93",
   "metadata": {},
   "source": [
    "## 5.2 Training an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9a6e277a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, \n",
    "                       num_epochs, eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # eval_freq表示多少步(step)后评估(evaluate)一次 eval_iter表示用多少个batch计算一次损失\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []  # track_tokens_seen在每次要评估的时候加入tokens_seen\n",
    "    tokens_seen, global_step = 0, -1  # tokens_seen记录每次循环后看到的tokens数\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()  # 每次要重置梯度，防止梯度累计\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            \n",
    "            loss.backward()  # 在此处反向传播更新梯度\n",
    "            \n",
    "            optimizer.step()  # 更新权重w\n",
    "            \n",
    "            tokens_seen += input_batch.numel()\n",
    "            \n",
    "            global_step += 1\n",
    "            \n",
    "            if global_step % eval_freq == 0:  # 当步数和预设相同\n",
    "                train_loss, val_loss = evaluate_model(model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)  # 一个列表，记录了每eval_freq步后的损失\n",
    "                val_losses.append(val_loss)\n",
    "                \n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                \n",
    "                print(f\"Ep {epoch + 1} (Step {global_step:06d}):\"\n",
    "                      f\"Train loss {train_loss:.3f},\"\n",
    "                      f\"Val loss {val_loss:.3f}\")\n",
    "                \n",
    "        generate_and_print_sample(model, tokenizer, device, start_context)  # 生成一些文本看效果\n",
    "                \n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d2a8667b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()  # 改为评价模式，消除dropout的影响\n",
    "    with torch.no_grad():  # 禁用梯度计算，节省算力\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()  # 恢复训练状态\n",
    "    return train_loss, val_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "09387a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()  # 推理状态同样关闭dropout\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(model=model, idx=encoded, max_new_tokens=50, context_size=context_size)\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    \n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    \n",
    "    model.train()  # 恢复训练状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3b4b798c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000):Train loss 9.820,Val loss 9.786\n",
      "Ep 1 (Step 000005):Train loss 7.921,Val loss 8.192\n",
      "Every effort moves you,,,,,,,,,,,,.                                     \n",
      "Ep 2 (Step 000010):Train loss 6.585,Val loss 6.740\n",
      "Ep 2 (Step 000015):Train loss 5.986,Val loss 5.995\n",
      "Every effort moves you, the, and, the, the, the, the. \", the, the,,, the, and, the,, the,, the, and, the, the,, the, and,,,, the,\n",
      "Ep 3 (Step 000020):Train loss 15.840,Val loss 15.627\n",
      "Ep 3 (Step 000025):Train loss 5.589,Val loss 5.469\n",
      "Every effort moves you. Gis. Gis. Gis. Gis. G. I had to----I--I, and--and, and I had. I had to the his--. Gis. Gisburn, and I had.\n",
      "Ep 4 (Step 000030):Train loss 5.357,Val loss 5.251\n",
      "Ep 4 (Step 000035):Train loss 4.778,Val loss 4.862\n",
      "Every effort moves you.                                                 \n",
      "Ep 5 (Step 000040):Train loss 4.278,Val loss 4.227\n",
      "Every effort moves you know the picture to have the picture--I was his pictures a little of his.                                 \n",
      "Ep 6 (Step 000045):Train loss 3.667,Val loss 3.854\n",
      "Ep 6 (Step 000050):Train loss 3.264,Val loss 3.293\n",
      "Every effort moves you know; and my a little of the donkey--I had a little of a little: \"--I looked up, I felt to see a little, I was a little, and in the fact, I felt, and, and he was his\n",
      "Ep 7 (Step 000055):Train loss 2.825,Val loss 2.777\n",
      "Ep 7 (Step 000060):Train loss 2.187,Val loss 2.269\n",
      "Every effort moves you know; and my dear, one of the deep arm-chairs forward. I had been his eyes.    \"Oh, in the moment--as Jack himself, as his pictures's his pictures, the man of the hour. \n",
      "Ep 8 (Step 000065):Train loss 1.816,Val loss 1.996\n",
      "Ep 8 (Step 000070):Train loss 1.429,Val loss 1.640\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the fact with the last word.        \"Oh, and I was back the head to the donkey. \"Oh, I was the donkey. \"I\n",
      "Ep 9 (Step 000075):Train loss 1.171,Val loss 1.263\n",
      "Ep 9 (Step 000080):Train loss 0.955,Val loss 0.910\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the fact with a little: \"Yes--and by me!\"  \"I didn't dabble back his head to look up at the honour being _mine_--because he's when I\n",
      "Ep 10 (Step 000085):Train loss 0.659,Val loss 0.729\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, token_seen = train_model_simple(model, train_loader, val_loader, optimizer, device,\n",
    "                                                         num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "                                                         start_context=\"Every effort moves you\", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4af30769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 [9.81980628967285, 7.921169471740723, 6.585442543029785, 5.985573863983154, 15.840124130249023, 5.589002513885498, 5.356519889831543, 4.777993202209473, 4.278285169601441, 3.6672969818115235, 3.2641485691070558, 2.824545478820801, 2.1874632835388184, 1.815918517112732, 1.429431104660034, 1.1708296537399292, 0.9548979043960572, 0.6591730356216431]\n"
     ]
    }
   ],
   "source": [
    "print(len(train_losses), train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "94a62d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 可视化训练结果\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "def plot_losses(epochs_seen, token_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(\n",
    "    epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\"\n",
    "    )\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax2 = ax1.twiny() \n",
    "    ax2.plot(token_seen, train_losses, alpha=0) \n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, token_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3133894",
   "metadata": {},
   "source": [
    "## 5.3 Decoding strategies to control randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5f58f8a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 回到cpu\n",
    "model.to(\"cpu\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e463a22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(\n",
    " model=model,\n",
    " idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    " max_new_tokens=25,\n",
    " context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915317ec",
   "metadata": {},
   "source": [
    "### 5.3.1 Temperature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9bf0af",
   "metadata": {},
   "source": [
    "在softmax前令logits / temperature, 如果temperature=1, 不变，temperature<1， 放大差距，结果更稳定，temperature>1，缩小差距，结果更灵活 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5a941899",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = { \n",
    " \"closer\": 0,\n",
    " \"every\": 1, \n",
    " \"effort\": 2, \n",
    " \"forward\": 3,\n",
    " \"inches\": 4,\n",
    " \"moves\": 5, \n",
    " \"pizza\": 6,\n",
    " \"toward\": 7,\n",
    " \"you\": 8,\n",
    "} \n",
    "inverse_vocab = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f8530d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_logits = torch.tensor(\n",
    " [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4c5c3b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e47c611a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toward\n"
     ]
    }
   ],
   "source": [
    "# multinomial按比例概率选择，结果是logits大的依然被选择，但不会稳定在一个\n",
    "torch.manual_seed(123) \n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b53acf31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "prob_dist must be 1 or 2 dim",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19936\\2600016148.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfreq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msampled_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{freq} x {inverse_vocab[i]}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mprint_sampled_tokens\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19936\\2600016148.py\u001b[0m in \u001b[0;36mprint_sampled_tokens\u001b[1;34m(probas)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mprint_sampled_tokens\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m123\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1_000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0msampled_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbincount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfreq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msampled_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19936\\2600016148.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mprint_sampled_tokens\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m123\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1_000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0msampled_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbincount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfreq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msampled_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: prob_dist must be 1 or 2 dim"
     ]
    }
   ],
   "source": [
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123)\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b77431",
   "metadata": {},
   "source": [
    "由于multinomial按比例计算，当使用temperature参数时，可以通过放大活缩小比例来选择灵活性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "89c07a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在softmax中加入temperature参数\n",
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a1a629bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temperatures = [1, 0.1, 5] \n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(x + i * bar_width, scaled_probas[i], bar_width, label=f'Temperature = {T}')\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cbd351",
   "metadata": {},
   "source": [
    "### 5.3.2 Top-k sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cfebc1",
   "metadata": {},
   "source": [
    "先选择logit最大的k个，再将其余logits设为无穷小，之后再softmax得到比例，这样非topk的prob为0，会在topk个token里选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "51189cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
      "Top positions: tensor([3, 7, 0])\n"
     ]
    }
   ],
   "source": [
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "print(\"Top logits:\", top_logits)\n",
    "print(\"Top positions:\", top_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1dc62b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n"
     ]
    }
   ],
   "source": [
    "new_logits = torch.where(\n",
    "     condition=next_token_logits < top_logits[-1], \n",
    "     input=torch.tensor(float('-inf')), \n",
    "     other=next_token_logits \n",
    "    )\n",
    "print(new_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5249ee3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "topk_probas = torch.softmax(new_logits, dim=0)\n",
    "print(topk_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e698b2e",
   "metadata": {},
   "source": [
    "### 5.3.3 Modifying the text generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c872cc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加入topk和temperature strategy后的生成函数\n",
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):  # eos指 end of sentence，是词表里的特殊符号\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]  # 选择生成的待选择序列\n",
    "        \n",
    "        # 加入top_k\n",
    "        if top_k is not None:\n",
    "            top_logits, _ = torch.topk(logits, top_k)  # 选择\n",
    "            min_val = top_logits[:, -1]\n",
    "            \n",
    "            logits = torch.where(logits < min_val, torch.tensor(float('-inf')).to(logits.device), logits)\n",
    "        \n",
    "        # 加入temperature\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        # 最原始的直接选最大logits\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        \n",
    "        if idx_next == eos_id:  # 如果遇到句子结束就停止\n",
    "            break\n",
    "            \n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f00bae01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you know began to happen. It was not it was such not to his history\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    "    )\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6b9b9b",
   "metadata": {},
   "source": [
    "## 5.4 Loading and saving model weights in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ca1c82c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 存储训练后的参数\n",
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1e6cf083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件大小: 622.63 MB\n"
     ]
    }
   ],
   "source": [
    "# 查看参数大小\n",
    "import os\n",
    "\n",
    "file_path = \"model.pth\"\n",
    "size_bytes = os.path.getsize(file_path)\n",
    "size_mb = size_bytes / (1024 * 1024)  # 转换为 MB\n",
    "\n",
    "print(f\"文件大小: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "84868f3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将存储结果应用在新建立的模型上\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(torch.load(\"model.pth\", map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9380a5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 同时存储权重和存储优化器参数，存储优化器参数为了接着训练时使用之前学习的结果\n",
    "torch.save({\n",
    " \"model_state_dict\": model.state_dict(),\n",
    " \"optimizer_state_dict\": optimizer.state_dict(),\n",
    " }, \n",
    " \"model_and_optimizer.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "652a640f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可以开始继续训练\n",
    "checkpoint = torch.load(\"model_and_optimizer.pth\", map_location=device)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02f7c5b",
   "metadata": {},
   "source": [
    "## 5.5 Loading pretrained weights from OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2155a14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow>=2.15.0 tqdm>=4.66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62bacd4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gpt_download.py', <http.client.HTTPMessage at 0x2247ae420d0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 下载模型权重的下载文件\n",
    "import urllib.request\n",
    "url = (\n",
    " \"https://raw.githubusercontent.com/rasbt/\"\n",
    " \"LLMs-from-scratch/main/ch05/\"\n",
    " \"01_main-chapter-code/gpt_download.py\"\n",
    ")\n",
    "filename = url.split('/')[-1]\n",
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01fd76f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\anaconda1\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 2.0.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"D:\\anaconda\\anaconda1\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n",
      "    app.start()\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n",
      "    self.io_loop.start()\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n",
      "    self._run_once()\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n",
      "    handle._run()\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n",
      "    await result\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 390, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2914, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2960, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 78, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3185, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3377, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3457, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\wangpeiru\\AppData\\Local\\Temp\\ipykernel_22304\\2555543219.py\", line 2, in <module>\n",
      "    from gpt_download import download_and_load_gpt2\n",
      "  File \"C:\\Users\\wangpeiru\\Desktop\\ai for material\\build_llm\\gpt_download.py\", line 13, in <module>\n",
      "    import tensorflow as tf\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\tensorflow\\__init__.py\", line 49, in <module>\n",
      "    from tensorflow._api.v2 import __internal__\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\__init__.py\", line 13, in <module>\n",
      "    from tensorflow._api.v2.__internal__ import feature_column\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\feature_column\\__init__.py\", line 8, in <module>\n",
      "    from tensorflow.python.feature_column.feature_column_v2 import DenseColumn # line: 1777\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column_v2.py\", line 38, in <module>\n",
      "    from tensorflow.python.feature_column import feature_column as fc_old\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column.py\", line 41, in <module>\n",
      "    from tensorflow.python.layers import base\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 16, in <module>\n",
      "    from tensorflow.python.keras.legacy_tf_layers import base\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\tensorflow\\python\\keras\\__init__.py\", line 25, in <module>\n",
      "    from tensorflow.python.keras import models\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\tensorflow\\python\\keras\\models.py\", line 25, in <module>\n",
      "    from tensorflow.python.keras.engine import training_v1\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py\", line 46, in <module>\n",
      "    from tensorflow.python.keras.engine import training_arrays_v1\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays_v1.py\", line 37, in <module>\n",
      "    from scipy.sparse import issparse  # pylint: disable=g-import-not-at-top\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\scipy\\sparse\\__init__.py\", line 267, in <module>\n",
      "    from ._csr import *\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\scipy\\sparse\\_csr.py\", line 10, in <module>\n",
      "    from ._sparsetools import (csr_tocsc, csr_tobsr, csr_count_blocks,\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"D:\\anaconda\\anaconda1\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n",
      "    app.start()\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n",
      "    self.io_loop.start()\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n",
      "    self._run_once()\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n",
      "    handle._run()\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n",
      "    await result\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 390, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2914, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2960, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 78, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3185, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3377, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3457, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\wangpeiru\\AppData\\Local\\Temp\\ipykernel_22304\\2555543219.py\", line 2, in <module>\n",
      "    from gpt_download import download_and_load_gpt2\n",
      "  File \"C:\\Users\\wangpeiru\\Desktop\\ai for material\\build_llm\\gpt_download.py\", line 13, in <module>\n",
      "    import tensorflow as tf\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\tensorflow\\__init__.py\", line 49, in <module>\n",
      "    from tensorflow._api.v2 import __internal__\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\__init__.py\", line 13, in <module>\n",
      "    from tensorflow._api.v2.__internal__ import feature_column\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\feature_column\\__init__.py\", line 8, in <module>\n",
      "    from tensorflow.python.feature_column.feature_column_v2 import DenseColumn # line: 1777\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column_v2.py\", line 38, in <module>\n",
      "    from tensorflow.python.feature_column import feature_column as fc_old\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column.py\", line 41, in <module>\n",
      "    from tensorflow.python.layers import base\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 16, in <module>\n",
      "    from tensorflow.python.keras.legacy_tf_layers import base\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\tensorflow\\python\\keras\\__init__.py\", line 25, in <module>\n",
      "    from tensorflow.python.keras import models\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\tensorflow\\python\\keras\\models.py\", line 25, in <module>\n",
      "    from tensorflow.python.keras.engine import training_v1\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py\", line 71, in <module>\n",
      "    from scipy.sparse import issparse  # pylint: disable=g-import-not-at-top\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\scipy\\sparse\\__init__.py\", line 267, in <module>\n",
      "    from ._csr import *\n",
      "  File \"D:\\anaconda\\anaconda1\\lib\\site-packages\\scipy\\sparse\\_csr.py\", line 10, in <module>\n",
      "    from ._sparsetools import (csr_tocsc, csr_tobsr, csr_count_blocks,\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22304\\2555543219.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 下载gpt2的权重\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgpt_download\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdownload_and_load_gpt2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m settings, params = download_and_load_gpt2(\n\u001b[0;32m      4\u001b[0m     \u001b[0mmodel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"124M\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodels_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"gpt2\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m )\n",
      "\u001b[1;32m~\\Desktop\\ai for material\\build_llm\\gpt_download.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\anaconda1\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    466\u001b[0m     \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tf_keras.src.optimizers\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m     \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"keras.src.optimizers\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m   \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\anaconda1\\lib\\importlib\\__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\anaconda1\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \"\"\"\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_tf_keras\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_tf_keras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mactivations\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapplications\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mapplications\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\anaconda1\\lib\\site-packages\\keras\\_tf_keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tf_keras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\anaconda\\anaconda1\\lib\\site-packages\\keras\\_tf_keras\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \"\"\"\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mactivations\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapplications\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mapplications\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\anaconda1\\lib\\site-packages\\keras\\activations\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \"\"\"\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivations\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeserialize\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivations\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivations\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mserialize\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mserialize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\anaconda1\\lib\\site-packages\\keras\\src\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\anaconda1\\lib\\site-packages\\keras\\src\\activations\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivations\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi_export\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras_export\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaving\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mobject_registration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaving\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mserialization_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\anaconda1\\lib\\site-packages\\keras\\src\\saving\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaving\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobject_registration\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_registered_object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaving\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobject_registration\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mregister_keras_serializable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaving\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaving_api\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaving\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserialization_lib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeserialize_keras_object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaving\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserialization_lib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mserialize_keras_object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\anaconda1\\lib\\site-packages\\keras\\src\\saving\\saving_api.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi_export\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras_export\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaving\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlegacy_h5_format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaving\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msaving_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfile_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\anaconda1\\lib\\site-packages\\keras\\src\\legacy\\saving\\legacy_h5_format.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaving\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaving\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msaving_options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaving\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msaving_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaving\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mobject_registration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\anaconda1\\lib\\site-packages\\keras\\src\\legacy\\saving\\saving_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmetrics_module\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\anaconda1\\lib\\site-packages\\keras\\src\\models\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFunctional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequential\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\anaconda1\\lib\\site-packages\\keras\\src\\models\\functional.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaving\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msaving_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaving\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mserialization\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mlegacy_serialization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFunction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_build_map\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\anaconda1\\lib\\site-packages\\keras\\src\\models\\model.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_mapping\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmap_saveable_variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaving\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msaving_api\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mbase_trainer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msummary_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtraceback_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\anaconda1\\lib\\site-packages\\keras\\src\\trainers\\trainer.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCompileLoss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCompileMetrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_adapters\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata_adapter_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpython_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtraceback_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\anaconda1\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribution\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistribution_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_adapters\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0marray_data_adapter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_adapters\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_adapters\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpy_dataset_adapter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\anaconda1\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\array_data_adapter.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_adapters\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0marray_slicing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_adapters\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata_adapter_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_adapters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_adapter\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataAdapter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\anaconda1\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\array_slicing.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mpandas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\anaconda1\\lib\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# numpy compat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mis_numpy_dev\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_is_numpy_dev\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\anaconda1\\lib\\site-packages\\pandas\\compat\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_typing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m from pandas.compat.numpy import (\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mis_numpy_dev\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mnp_version_under1p19\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\anaconda1\\lib\\site-packages\\pandas\\compat\\numpy\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mVersion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# numpy versioning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\anaconda1\\lib\\site-packages\\pandas\\util\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m from pandas.util._decorators import (  # noqa:F401\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mAppender\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mSubstitution\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mcache_readonly\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m )\n",
      "\u001b[1;32mD:\\anaconda\\anaconda1\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_libs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproperties\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcache_readonly\u001b[0m  \u001b[1;31m# noqa:F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_typing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\anaconda1\\lib\\site-packages\\pandas\\_libs\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_libs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterval\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInterval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m from pandas._libs.tslibs import (\n\u001b[0;32m     15\u001b[0m     \u001b[0mNaT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\anaconda1\\lib\\site-packages\\pandas\\_libs\\interval.pyx\u001b[0m in \u001b[0;36minit pandas._libs.interval\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "# 下载gpt2的权重\n",
    "from gpt_download import download_and_load_gpt2\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=\"124M\", models_dir=\"gpt2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b05defc8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9ced2304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hf_gpt2_to_textbook_params(model_name: str = \"gpt2\"):\n",
    "    \"\"\"\n",
    "    将 Hugging Face 的 GPT-2 模型转换为教材 load_weights_into_gpt 所需的 (settings, params) 结构。\n",
    "    适配以下键名：wte, wpe, g, b, blocks[i].attn.c_attn.{w,b}, blocks[i].attn.c_proj.{w,b},\n",
    "                 blocks[i].mlp.c_fc.{w,b}, blocks[i].mlp.c_proj.{w,b}, blocks[i].ln_1.{g,b}, ln_2.{g,b}\n",
    "    \"\"\"\n",
    "    tok = AutoTokenizer.from_pretrained(model_name)\n",
    "    mdl = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    sd = mdl.state_dict()\n",
    "    cfg = mdl.config\n",
    "\n",
    "    settings = {\n",
    "        \"n_vocab\": cfg.vocab_size,\n",
    "        \"n_ctx\":   cfg.n_positions,\n",
    "        \"n_embd\":  cfg.n_embd,\n",
    "        \"n_head\":  cfg.n_head,\n",
    "        \"n_layer\": cfg.n_layer,\n",
    "        \"activation_function\": getattr(cfg, \"activation_function\", \"gelu\"),\n",
    "        \"layer_norm_epsilon\":  getattr(cfg, \"layer_norm_epsilon\", 1e-5),\n",
    "    }\n",
    "\n",
    "    to_np = lambda t: t.detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "    params = {}\n",
    "    # embeddings\n",
    "    params[\"wte\"] = to_np(sd[\"transformer.wte.weight\"])  # [vocab, n_embd]\n",
    "    params[\"wpe\"] = to_np(sd[\"transformer.wpe.weight\"])  # [n_ctx, n_embd]\n",
    "\n",
    "    # final layer norm -> 顶层 g/b（教材命名）\n",
    "    params[\"g\"] = to_np(sd[\"transformer.ln_f.weight\"])\n",
    "    params[\"b\"] = to_np(sd[\"transformer.ln_f.bias\"])\n",
    "\n",
    "    blocks = []\n",
    "    for i in range(cfg.n_layer):\n",
    "        blk = {}\n",
    "\n",
    "        # ---- Attention ----\n",
    "        blk.setdefault(\"attn\", {})\n",
    "        # fused qkv\n",
    "        blk[\"attn\"].setdefault(\"c_attn\", {})\n",
    "        W_qkv = to_np(sd[f\"transformer.h.{i}.attn.c_attn.weight\"])   # [n_embd, 3*n_embd]\n",
    "        b_qkv = to_np(sd[f\"transformer.h.{i}.attn.c_attn.bias\"])     # [3*n_embd]\n",
    "        # 教材稍后会 np.split(..., axis=-1) 并在赋值处 .T，这里保持 fused 形式\n",
    "        blk[\"attn\"][\"c_attn\"][\"w\"] = W_qkv\n",
    "        blk[\"attn\"][\"c_attn\"][\"b\"] = b_qkv\n",
    "\n",
    "        # out proj\n",
    "        blk[\"attn\"][\"c_proj\"] = {\n",
    "            \"w\": to_np(sd[f\"transformer.h.{i}.attn.c_proj.weight\"]),  # [n_embd, n_embd]  教材赋值时会 .T\n",
    "            \"b\": to_np(sd[f\"transformer.h.{i}.attn.c_proj.bias\"]),\n",
    "        }\n",
    "\n",
    "        # ---- MLP ----\n",
    "        blk.setdefault(\"mlp\", {})\n",
    "        blk[\"mlp\"][\"c_fc\"] = {\n",
    "            \"w\": to_np(sd[f\"transformer.h.{i}.mlp.c_fc.weight\"]),     # [n_embd, 4*n_embd] 教材赋值时 .T\n",
    "            \"b\": to_np(sd[f\"transformer.h.{i}.mlp.c_fc.bias\"]),\n",
    "        }\n",
    "        blk[\"mlp\"][\"c_proj\"] = {\n",
    "            \"w\": to_np(sd[f\"transformer.h.{i}.mlp.c_proj.weight\"]),   # [4*n_embd, n_embd] 教材赋值时 .T\n",
    "            \"b\": to_np(sd[f\"transformer.h.{i}.mlp.c_proj.bias\"]),\n",
    "        }\n",
    "\n",
    "        # ---- LayerNorms ----\n",
    "        blk[\"ln_1\"] = {\n",
    "            \"g\": to_np(sd[f\"transformer.h.{i}.ln_1.weight\"]),\n",
    "            \"b\": to_np(sd[f\"transformer.h.{i}.ln_1.bias\"]),\n",
    "        }\n",
    "        blk[\"ln_2\"] = {\n",
    "            \"g\": to_np(sd[f\"transformer.h.{i}.ln_2.weight\"]),\n",
    "            \"b\": to_np(sd[f\"transformer.h.{i}.ln_2.bias\"]),\n",
    "        }\n",
    "\n",
    "        blocks.append(blk)\n",
    "\n",
    "    params[\"blocks\"] = blocks\n",
    "    return settings, params, tok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "139cb7ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d06c726922084cd6a4cfe27edec10039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\anaconda1\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\wangpeiru\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "946d811aeb0e4fa5aeac62ecb8d3c9ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "settings, params, tok = hf_gpt2_to_textbook_params(\"gpt2\")  # 124M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16b6f890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12, 'activation_function': 'gelu_new', 'layer_norm_epsilon': 1e-05}\n",
      "Parameter dictionary keys: dict_keys(['wte', 'wpe', 'g', 'b', 'blocks'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Settings:\", settings)\n",
    "print(\"Parameter dictionary keys:\", params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5dfdf0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n",
      "   0.04531523]\n",
      " [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n",
      "   0.04318958]\n",
      " [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n",
      "  -0.08785918]\n",
      " ...\n",
      " [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n",
      "  -0.06952604]\n",
      " [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n",
      "  -0.02245961]\n",
      " [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n",
      "   0.12067825]]\n",
      "Token embedding weight tensor dimensions: (50257, 768)\n"
     ]
    }
   ],
   "source": [
    "print(params[\"wte\"])\n",
    "print(\"Token embedding weight tensor dimensions:\", params[\"wte\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b4816012",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = {\n",
    " \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    " \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    " \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    " \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "356d97c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2-small (124M)\"\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "aa3f9a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_CONFIG.update({\"context_length\": 1024})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6af566e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_CONFIG.update({\"qkv_bias\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c96191e2",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ab99e96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 判断左右两个输入维度是否相同，用于判断下载的权重和模型结构是否一致\n",
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, \"\n",
    "        \"Right: {right.shape}\"\n",
    "        )\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bc81367a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(param: torch.Tensor, array_like) -> torch.Tensor:\n",
    "    \"\"\"Safely copy numpy/torch data into an existing Parameter/buffer and return it.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        src = torch.as_tensor(array_like, dtype=param.dtype, device=param.device)\n",
    "        assert param.shape == src.shape, f\"shape mismatch: {param.shape} vs {src.shape}\"\n",
    "        param.copy_(src)  # in-place!\n",
    "    return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4865f911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2cfeda7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights_into_gpt(gpt, params): \n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "\n",
    "    for b in range(len(params[\"blocks\"])): \n",
    "#         q_w, k_w, v_w = np.split( \n",
    "#             (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "#         gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "#             gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "#         gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "#             gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "#         gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "#             gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "\n",
    "#         q_b, k_b, v_b = np.split(\n",
    "#             (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "#         gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "#             gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "#         gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "#             gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "#         gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "#             gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "        \n",
    "        q_w, k_w, v_w = np.split(params[\"blocks\"][b][\"attn\"][\"c_attn\"][\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_q.weight = assign(gpt.trf_blocks[b].att.W_q.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_k.weight = assign(gpt.trf_blocks[b].att.W_k.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_v.weight = assign(gpt.trf_blocks[b].att.W_v.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(params[\"blocks\"][b][\"attn\"][\"c_attn\"][\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_q.bias = assign(gpt.trf_blocks[b].att.W_q.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_k.bias = assign(gpt.trf_blocks[b].att.W_k.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_v.bias = assign(gpt.trf_blocks[b].att.W_v.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "    \n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bbeefcf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "20527201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " People love cats as much as dogs, although many humans (and not all) want their pets feline companions. Pets keep humans healthy,\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    " model=gpt,\n",
    " idx=text_to_token_ids(\"People love cats\", tokenizer).to(device),\n",
    " max_new_tokens=25,\n",
    " context_size=NEW_CONFIG[\"context_length\"],\n",
    " top_k=50,\n",
    " temperature=1.5\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7bad4bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 尝试应用gpt xl\n",
    "settings_xl, params_xl, tok_xl = hf_gpt2_to_textbook_params(\"gpt2-xl\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "eafad32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a254feee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 1600)\n",
       "  (pos_emb): Embedding(1024, 1600)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (12): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (13): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (14): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (15): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (16): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (17): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (18): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (19): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (20): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (21): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (22): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (23): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (24): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (25): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (26): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (27): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (28): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (29): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (30): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (31): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (32): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (33): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (34): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (35): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (36): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (37): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (38): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (39): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (40): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (41): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (42): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (43): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (44): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (45): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (46): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (47): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=1600, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gpt2xl配置文件\n",
    "GPT2XL_CONFIG = GPT_CONFIG_124M.copy()\n",
    "GPT2XL_CONFIG.update(model_configs[\"gpt2-xl (1558M)\"])\n",
    "GPT2XL_CONFIG.update({\"context_length\": 1024})\n",
    "GPT2XL_CONFIG.update({\"qkv_bias\": True})\n",
    "\n",
    "gpt2_xl = GPTModel(GPT2XL_CONFIG)\n",
    "gpt2_xl.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f06c4cf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 1600)\n",
       "  (pos_emb): Embedding(1024, 1600)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (12): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (13): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (14): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (15): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (16): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (17): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (18): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (19): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (20): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (21): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (22): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (23): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (24): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (25): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (26): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (27): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (28): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (29): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (30): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (31): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (32): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (33): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (34): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (35): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (36): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (37): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (38): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (39): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (40): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (41): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (42): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (43): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (44): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (45): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (46): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (47): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_k): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_v): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELUNew()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=1600, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 载入gpt2xl的权重\n",
    "load_weights_into_gpt(gpt2_xl, params_xl)\n",
    "gpt2_xl.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "798388c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " computer science students will get a job in the field.\n",
      "\n",
      "\"We're going to be able to help them get a job in the field,\" said Dr. David H. Hsu, a professor of computer science at the University of California, Berkeley. \"We're going to\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    " model=gpt2_xl,\n",
    " idx=text_to_token_ids(\"computer science students will get a job\", tokenizer).to(device).long(),\n",
    " max_new_tokens=50,\n",
    " context_size=GPT2XL_CONFIG[\"context_length\"],\n",
    " top_k=None,\n",
    " temperature=0.0\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ab8195c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "页面文件太小，无法完成操作。 (os error 1455)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21776\\440763616.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# hf = AutoModelForCausalLM.from_pretrained(\"gpt2-xl\").to(\"cpu\").eval()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m hf = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[0;32m      5\u001b[0m                                         \u001b[1;34m\"gpt2-xl\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m                                         \u001b[0mlow_cpu_mem_usage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\anaconda1\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    602\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"text_config\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    603\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 604\u001b[1;33m             return model_class.from_pretrained(\n\u001b[0m\u001b[0;32m    605\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    606\u001b[0m             )\n",
      "\u001b[1;32mD:\\anaconda\\anaconda1\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    289\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\anaconda1\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   5055\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mcheckpoint_files\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".safetensors\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5056\u001b[0m         ):\n\u001b[1;32m-> 5057\u001b[1;33m             \u001b[1;32mwith\u001b[0m \u001b[0msafe_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint_files\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframework\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"pt\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5058\u001b[0m                 \u001b[0mmetadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5059\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: 页面文件太小，无法完成操作。 (os error 1455)"
     ]
    }
   ],
   "source": [
    "# 查看模型是否正确加载\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# hf = AutoModelForCausalLM.from_pretrained(\"gpt2-xl\").to(\"cpu\").eval()\n",
    "hf = AutoModelForCausalLM.from_pretrained(\n",
    "                                        \"gpt2-xl\",\n",
    "                                        low_cpu_mem_usage=True\n",
    "                                    ).to(\"cpu\").eval()\n",
    "gpt2_xl_cpu = gpt2_xl.to(\"cpu\").eval()  \n",
    "\n",
    "ids = text_to_token_ids(\"People love cats\", tokenizer).to(\"cpu\").long()\n",
    "with torch.no_grad():\n",
    "    my_logits = gpt2_xl_cpu(ids)[:, -1, :]\n",
    "    hf_logits = hf(ids).logits[:, -1, :]\n",
    "\n",
    "print(\"max |Δlogit| =\", (my_logits - hf_logits).abs().max().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4f05c9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids_hf: [[8061, 1842, 11875]]\n",
      "ids_mine: [[8061, 1842, 11875]]\n",
      "ids_equal? -> True\n",
      "Δ wte: 5.485938549041748\n",
      "Δ wpe: 4.849704742431641\n",
      "Δ embeds: 5.983179569244385\n",
      "Δ block0: 23.41231918334961\n",
      "First big diff at block 0\n",
      "Δ ln_f: 27.681869506835938\n",
      "Δ logits: 76.47346496582031\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = \"cpu\"  # 避免 OOM，做对比用 CPU\n",
    "hf = AutoModelForCausalLM.from_pretrained(\"gpt2-xl\").to(device).eval()\n",
    "hf_tok = AutoTokenizer.from_pretrained(\"gpt2-xl\")\n",
    "\n",
    "prompt = \"People love cats\"\n",
    "\n",
    "# 1) 用 HF tokenizer 与你当前 tokenizer 分别编码\n",
    "ids_hf = torch.tensor([hf_tok.encode(prompt, add_special_tokens=False)], device=device)\n",
    "ids_mine = text_to_token_ids(prompt, tokenizer)  # 你现在使用的编码器\n",
    "ids_mine = ids_mine.to(device).long()\n",
    "\n",
    "print(\"ids_hf:\", ids_hf.tolist())\n",
    "print(\"ids_mine:\", ids_mine.tolist())\n",
    "print(\"ids_equal? ->\", torch.equal(ids_hf, ids_mine))\n",
    "\n",
    "# 2) 如果不相等，就用 HF 的 ids 做全部对比\n",
    "ids = ids_hf\n",
    "\n",
    "@torch.no_grad()\n",
    "def emb_my(m, ids):\n",
    "    T = ids.size(1)\n",
    "    return m.tok_emb(ids) + m.pos_emb(torch.arange(T, device=ids.device))\n",
    "\n",
    "@torch.no_grad()\n",
    "def emb_hf(m, ids):\n",
    "    T = ids.size(1)\n",
    "    return m.transformer.wte(ids) + m.transformer.wpe(torch.arange(T, device=ids.device))\n",
    "\n",
    "with torch.no_grad():\n",
    "    # 先检查权重是否加载正确（直接比权重本身）\n",
    "    print(\"Δ wte:\", (gpt2_xl.tok_emb.weight.cpu() - hf.transformer.wte.weight.cpu()).abs().max().item())\n",
    "    print(\"Δ wpe:\", (gpt2_xl.pos_emb.weight.cpu() - hf.transformer.wpe.weight.cpu()).abs().max().item())\n",
    "\n",
    "    x_my = emb_my(gpt2_xl.to(device).float().eval(), ids)\n",
    "    x_hf = emb_hf(hf, ids)\n",
    "    print(\"Δ embeds:\", (x_my - x_hf).abs().max().item())\n",
    "\n",
    "    # 逐层对比（修正 tuple 取法）\n",
    "    x1, x2 = x_my, x_hf\n",
    "    for i in range(len(gpt2_xl.trf_blocks)):\n",
    "        y1 = gpt2_xl.trf_blocks[i](x1)\n",
    "        y2 = hf.transformer.h[i](x2)[0]\n",
    "        diff = (y1 - y2).abs().max().item()\n",
    "        print(f\"Δ block{i}:\", diff)\n",
    "        if diff > 1e-2:\n",
    "            print(\"First big diff at block\", i); break\n",
    "        x1, x2 = y1, y2\n",
    "\n",
    "    out1 = gpt2_xl.final_norm(x1)\n",
    "    out2 = hf.transformer.ln_f(x2)\n",
    "    print(\"Δ ln_f:\", (out1 - out2).abs().max().item())\n",
    "\n",
    "    my_logits = gpt2_xl.out_head(out1)[:, -1, :]\n",
    "    hf_logits = hf.lm_head(out2)[:, -1, :]\n",
    "    print(\"Δ logits:\", (my_logits - hf_logits).abs().max().item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
